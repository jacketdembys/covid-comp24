{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ceec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, cv2, math, copy, time, random\n",
    "import pickle\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.cuda import amp\n",
    "import torch.backends.cudnn as cudnn\n",
    "import threading\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import ast\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6526bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=8\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32beb92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, scheduler, dataloader, device, epoch):\n",
    "    model.train()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    print(f'Epoch:{epoch}')\n",
    "    \n",
    "    for step, data in bar:\n",
    "        ct_b, img_b, c, h, w = data['image'].size()\n",
    "        data_img = data['image'].reshape(-1, c, h, w)\n",
    "        data_label = data['label'].reshape(-1,1)\n",
    "        images = data_img.to(device, dtype=torch.float)\n",
    "        labels = data_label.to(device, dtype=torch.float)\n",
    "\n",
    "        \n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        with amp.autocast(enabled = True):\n",
    "\n",
    "            \n",
    "            outputs = model(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6de4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    true_y=[]\n",
    "    pred_y=[]\n",
    "    for step, data in bar:\n",
    "        ct_b, img_b, c, h, w = data['image'].size()\n",
    "        data_img = data['image'].reshape(-1, c, h, w)\n",
    "        data_label = data['label'].reshape(-1,1)\n",
    "\n",
    "        images = data_img.to(device, dtype=torch.float)\n",
    "        labels = data_label.to(device, dtype=torch.float)\n",
    "\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "\n",
    "        true_y.append(labels.cpu().numpy())\n",
    "        pred_y.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "\n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        dataset_size += batch_size\n",
    "\n",
    "        epoch_loss = running_loss / dataset_size\n",
    "\n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "\n",
    "    true_y=np.concatenate(true_y)\n",
    "    pred_y=np.concatenate(pred_y)\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    true_y=np.array(true_y).reshape(-1,1)\n",
    "    true_y=np.array(true_y).reshape(-1,img_b)\n",
    "    true_y=true_y.mean(axis=1)\n",
    "\n",
    "    pred_y=np.array(pred_y).reshape(-1,1)\n",
    "    pred_y = torch.nan_to_num(torch.from_numpy(pred_y)).numpy()\n",
    "    pred_y=np.array(pred_y).reshape(-1,img_b)\n",
    "\n",
    "    pred_y=pred_y.mean(axis=1)\n",
    "\n",
    "    assert(true_y.ndim==1)\n",
    "    assert(pred_y.ndim==1)\n",
    "    assert (true_y.shape==pred_y.shape)\n",
    "    \n",
    "    acc_f1=f1_score(np.array(true_y),np.round(pred_y),average='macro')\n",
    "    auc_roc=roc_auc_score(np.array(true_y),np.array(pred_y))\n",
    "    print(\"acc_f1(mean) : \",round(acc_f1,4),\"  auc_roc(mean) : \",round(auc_roc,4))\n",
    "\n",
    "\n",
    "    return epoch_loss,acc_f1,auc_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2421f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training(model, optimizer, scheduler, device, num_epochs):\n",
    "\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_loss = np.inf\n",
    "    best_epoch_auc = 0\n",
    "    best_epoch_f1 = 0\n",
    "    history = defaultdict(list)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        gc.collect()\n",
    "        train_epoch_loss = train_one_epoch(model, optimizer, scheduler, dataloader=train_loader, device=CONFIG['device'], epoch=epoch)\n",
    "        val_epoch_loss,acc_f1,auc_roc= valid_one_epoch(model, valid_loader, device=CONFIG['device'],epoch=epoch)\n",
    "\n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "\n",
    "\n",
    "        if val_epoch_loss <= best_epoch_loss:\n",
    "            print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_epoch_loss})\")\n",
    "            best_epoch_loss = val_epoch_loss\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f'{bin_save_path}/loss_ECA/'+job_name\n",
    "            os.makedirs(f'{bin_save_path}/loss_ECA/', exist_ok=True)\n",
    "            torch.save(model.module.state_dict(), PATH)\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "        if auc_roc >= best_epoch_auc:\n",
    "            print(f\"Validation Auc Improved ({best_epoch_auc} ---> {auc_roc})\")\n",
    "            best_epoch_auc = auc_roc\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f'{bin_save_path}/auc_roc_ECA/'+job_name\n",
    "            os.makedirs(f'{bin_save_path}/auc_roc_ECA/', exist_ok=True)\n",
    "            torch.save(model.module.state_dict(), PATH)\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "        if acc_f1 >= best_epoch_f1:\n",
    "            print(f\"Validation f1 Improved ({best_epoch_f1} ---> {acc_f1})\")\n",
    "            best_epoch_f1 = acc_f1\n",
    "\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f'{bin_save_path}/f1_ECA/'+job_name\n",
    "            os.makedirs(f'{bin_save_path}/f1_ECA/', exist_ok=True)\n",
    "            torch.save(model.module.state_dict(), PATH)\n",
    "            print(f\"Model Saved\")\n",
    "\n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb42bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "        \"epochs\":  100, \n",
    "        \"img_size\": 384,\n",
    "        \"valid_batch_size\": 8,\n",
    "        \"learning_rate\": 0.0001,\n",
    "\n",
    "        \"weight_decay\": 0.0005, \n",
    "\n",
    "        \"n_accumulate\": 1, \n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e40d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "\"train\": A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.ShiftScaleRotate(shift_limit=0.2, \n",
    "                       scale_limit=0.2, \n",
    "                       rotate_limit=30, \n",
    "                       p=0.5),\n",
    "    A.HueSaturationValue(\n",
    "            hue_shift_limit=0.2, \n",
    "            sat_shift_limit=0.2, \n",
    "            val_shift_limit=0.2, \n",
    "            p=0.5 \n",
    "        ),\n",
    "    A.RandomBrightnessContrast(\n",
    "            brightness_limit=(-0.2,0.2), #0.2\n",
    "            contrast_limit=(-0.2, 0.2),  #0.2\n",
    "            p=0.5 \n",
    "        ),\n",
    "    A.dropout.coarse_dropout.CoarseDropout(p=0.2),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()], p=1.),\n",
    "\n",
    "\"valid\": A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()], p=1.)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b21064",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir='/mnt/challenge_preprocessing/files/filtered'\n",
    "\n",
    "challenge1_train = pd.read_csv(os.path.join(files_dir,'filtered-la-challenge1_train_path_range_label.csv'))\n",
    "challenge1_valid = pd.read_csv(os.path.join(files_dir,'filtered-la-challenge1_valid_path_range_label.csv'))\n",
    "\n",
    "challenge2_train = pd.read_csv(os.path.join(files_dir,'filtered-la-challenge2_train_path_range_label.csv'))\n",
    "challenge2_valid = pd.read_csv(os.path.join(files_dir,'filtered-la-challenge2_valid_path_range_label.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7766a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge_valid = pd.concat([challenge1_valid, challenge2_valid], axis=0)\n",
    "challenge_train=  pd.concat([challenge1_train, challenge2_train, challenge2_valid], axis=0)\n",
    "challenge_train=  challenge_train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#challenge_train=challenge1_train\n",
    "challenge_valid=challenge1_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd9bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None, batch_size=16):\n",
    "        self.img_paths = df['Path'].tolist()\n",
    "        self.labels = df['Label'].values\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.ranges = df['Range'].apply(ast.literal_eval).tolist()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        range_list = self.ranges[idx]\n",
    "        \n",
    "        if '/challenge1' in img_path:\n",
    "            img_path = img_path.replace('preprocessed', 'OriginalDatauncompressed')\n",
    "        else:\n",
    "            img_path = img_path.replace('preprocessed', 'dataset')\n",
    "         \n",
    "        if ('neg' in img_path and label == 1) or ('pos' in img_path and label == 0) or ('/co' in img_path and label == 0) or ('/no' in img_path and label == 1):\n",
    "            print(img_path, label)\n",
    "        \n",
    "        file_paths = list(range(range_list[0], range_list[1]))\n",
    "        \n",
    "        if len(file_paths) > self.batch_size:\n",
    "            sampled_paths = np.random.choice(file_paths, size=self.batch_size, replace=False)       \n",
    "        else:\n",
    "            sampled_paths = file_paths\n",
    "        sampled_paths = np.sort(sampled_paths)\n",
    "        \n",
    "        images = torch.empty((self.batch_size, 3, 384, 384))\n",
    "        labels = torch.empty((self.batch_size,1))    \n",
    "        \n",
    "        for i, path in enumerate(sampled_paths):\n",
    "            img = cv2.imread(os.path.join(img_path, f\"{path}.jpg\"))\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            img = self.transforms(image=img)['image']\n",
    "            images[i] = img[:]\n",
    "            labels[i] = label\n",
    "            \n",
    "\n",
    "        return {\n",
    "            'image': images,\n",
    "            'label': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b82ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=ChallengeDataset(challenge_train,data_transforms['train'],12)\n",
    "valid_dataset=ChallengeDataset(challenge_valid,data_transforms['valid'],12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90ad8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=14,  shuffle=False, pin_memory=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=14,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1dbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eca_nfnet_l0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(eca_nfnet_l0, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"hf_hub:timm/eca_nfnet_l0\", pretrained=True)\n",
    "        self.classifier = nn.Linear(self.model.head.fc.in_features, 1, bias=True)\n",
    "        \n",
    "        self.attention = nn.Conv2d(2, 1, kernel_size=1, bias=True)\n",
    "        \n",
    "        layer_name = 'final_conv'\n",
    "        \n",
    "        self.features = {}\n",
    "        \n",
    "        self.model.final_act.register_forward_hook(self.get_features)\n",
    "\n",
    "    def set_features(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def get_features(self, module, input, output):\n",
    "        self.features[threading.get_ident()] = output\n",
    "\n",
    "    def getAttFeats(self, att_map, features):\n",
    "        features = 0.5 * features + 0.5 * (att_map * features)\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        \n",
    "        dummy = self.model(x)\n",
    "        \n",
    "        features = self.features[threading.get_ident()]\n",
    "        fg_att = self.attention(torch.cat((torch.mean(features, dim=1).unsqueeze(1), torch.max(features, dim=1)[0].unsqueeze(1)), dim=1))\n",
    "        fg_att = torch.sigmoid(fg_att)\n",
    "        features = self.getAttFeats(fg_att, features)\n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        outputs['logits'] = out\n",
    "        outputs['feat'] = features\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "bin_save_path = \"/mnt/saved_models/14batch_eachof12_combing_challenge1_2training_challene2validation\"\n",
    "job_name = f\"epoch:{CONFIG['epochs']}_ECA_Attention_{CONFIG['img_size']}\"\n",
    "model = eca_nfnet_l0()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.BCEWithLogitsLoss()\n",
    "model = nn.DataParallel(model)#, device_ids=[0,1,2,3])\n",
    "\n",
    "model = model.to(CONFIG['device'])\n",
    "\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "print(\"=\"*10, \"*model* setting\", \"=\"*10)\n",
    "optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "print(\"=\"*10, \"Start Train\", \"=\"*10)\n",
    "model, history= run_training(model, optimizer,None, device=CONFIG['device'], num_epochs=CONFIG['epochs'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-UnetCRF2] *",
   "language": "python",
   "name": "conda-env-.conda-UnetCRF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
