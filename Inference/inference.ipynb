{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5da843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, cv2, math, copy, time, random\n",
    "import pickle\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.cuda import amp\n",
    "import torch.backends.cudnn as cudnn\n",
    "import threading\n",
    "\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import f1_score,roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import ast\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ccf076",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "        \"img_size\": 384,\n",
    "        \"valid_batch_size\":1,\n",
    "        \"device\": torch.device(\"cuda\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c83ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms =  A.Compose([\n",
    "    A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()], p=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa94debf",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_dir='/home/Data/rmf3mc/Challenge/Challenge1_Testset/files'\n",
    "test_set = pd.read_csv(os.path.join(files_dir,'la-test_path_range.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789a75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChallengeDataset(Dataset):\n",
    "    def __init__(self, df, transforms=None, batch_size=16):\n",
    "\n",
    "        self.img_paths = df['Path'].tolist()\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        self.ranges = df['Range'].apply(ast.literal_eval).tolist()\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        # Get the image path and label for the current index\n",
    "        img_path = self.img_paths[idx]\n",
    "        range_list= self.ranges[idx]\n",
    "        \n",
    "\n",
    "        img_path = img_path.replace('test_set_preprocessed', 'test_set')\n",
    "         \n",
    "              \n",
    "        sampled_paths = np.round(np.linspace(range_list[0], range_list[1], self.batch_size, endpoint=False)).astype(int)\n",
    "#         images = torch.empty((self.batch_size, 3, 384, 384))  # Example size: [3, 224, 224]\n",
    "        images=[]\n",
    "        \n",
    "        for i, path in enumerate(sampled_paths):\n",
    "            img = cv2.imread(os.path.join(img_path, f\"{path}.jpg\") )\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            \n",
    "            img = self.transforms(image=img)['image']\n",
    "            tensor = torch.tensor(img)\n",
    "            \n",
    "            images.append(tensor)\n",
    "        tensor=torch.stack(images)\n",
    "        return {\n",
    "            'image': tensor, 'path': img_path,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437a950",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=ChallengeDataset(test_set,data_transforms,40)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1,  shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class eca_nfnet_l0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(eca_nfnet_l0, self).__init__()\n",
    "\n",
    "        self.model = timm.create_model(\"hf_hub:timm/eca_nfnet_l0\", pretrained=True)\n",
    "        self.classifier = nn.Linear(self.model.head.fc.in_features, 1, bias=True)\n",
    "        \n",
    "        self.attention = nn.Conv2d(2, 1, kernel_size=1, bias=True)\n",
    "        \n",
    "        layer_name = 'final_conv'\n",
    "        \n",
    "        self.features = {}\n",
    "        \n",
    "        self.model.final_act.register_forward_hook(self.get_features)\n",
    "\n",
    "    def set_features(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def get_features(self, module, input, output):\n",
    "        self.features[threading.get_ident()] = output\n",
    "\n",
    "    def getAttFeats(self, att_map, features):\n",
    "        features = 0.5 * features + 0.5 * (att_map * features)\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = {}\n",
    "        \n",
    "        dummy = self.model(x)\n",
    "        \n",
    "        features = self.features[threading.get_ident()]\n",
    "        fg_att = self.attention(torch.cat((torch.mean(features, dim=1).unsqueeze(1), torch.max(features, dim=1)[0].unsqueeze(1)), dim=1))\n",
    "        fg_att = torch.sigmoid(fg_att)\n",
    "        features = self.getAttFeats(fg_att, features)\n",
    "        \n",
    "        out = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        outputs['logits'] = out\n",
    "        outputs['feat'] = features\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    \n",
    "bin_save_path = \"/mnt/saved_models/14batch_eachof12_combing_challenge1_2training_challene2validation\"\n",
    "job_name = f\"epoch:{CONFIG['epochs']}_ECA_Attention_{CONFIG['img_size']}\"\n",
    "model = eca_nfnet_l0()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a49240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=os.path.join(files_dir,'model.pth')\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b0c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621927bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_covid=[]\n",
    "covid=[]\n",
    "@torch.inference_mode()\n",
    "def valid_one_epoch(model, dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    pred_y=[]\n",
    "    \n",
    "    for step, data in bar:\n",
    "        ct_b, img_b, c, h, w = data['image'].size()\n",
    "        data_img = data['image'].reshape(-1, c, h, w)\n",
    "        path=data['path'][0]\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        \n",
    "        images = data_img.to(device, dtype=torch.float)\n",
    "\n",
    "\n",
    "        outputs = model(images)\n",
    "\n",
    "        pred_y=torch.sigmoid(outputs).cpu().numpy()\n",
    "        if pred_y.mean()<0.5:\n",
    "            non_covid.append(file_name)\n",
    "        else:\n",
    "            covid.append(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0add6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_one_epoch(model, test_loader, CONFIG['device'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a063e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(os.path.join(files_dir,'non-covid.csv'), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for element in non_covid:\n",
    "        writer.writerow([element])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c66fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(files_dir,'covid.csv'), mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    for element in covid:\n",
    "        writer.writerow([element])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-UnetCRF2] *",
   "language": "python",
   "name": "conda-env-.conda-UnetCRF2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
